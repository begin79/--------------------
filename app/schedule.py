import re
import logging
import datetime
from typing import List, Tuple, Optional, Literal, Dict
from bs4 import BeautifulSoup
from urllib.parse import quote
from cachetools import TTLCache

from .config import BASE_URL_SCHEDULE, BASE_URL_LIST
from .constants import (
    API_TYPE_GROUP,
    API_TYPE_TEACHER,
    GROUP_NAME_PATTERN,
    SUBGROUP_PATTERN,
    PAIR_EMOJIS,
)
from .http import make_request_with_retry

logger = logging.getLogger(__name__)

# –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–µ—à–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
schedule_cache = TTLCache(maxsize=500, ttl=600)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100 –¥–æ 500
list_cache = TTLCache(maxsize=50, ttl=3600)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 10 –¥–æ 50

def parse_date_from_html(day_date_str: str) -> Optional[datetime.date]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏ HTML (–Ω–∞–ø—Ä–∏–º–µ—Ä, "03.11.2025", "11 –Ω–æ—è–±—Ä—è 2025" –∏–ª–∏ "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 03.11.2025")
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç date –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
    """
    if not day_date_str:
        return None

    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
        date_match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', day_date_str)
        if date_match:
            day, month, year = map(int, date_match.groups())
            result = datetime.date(year, month, day)
            logger.debug(f"–†–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ –¥–∞—Ç–∞ –∏–∑ '{day_date_str}': {result}")
            return result

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ —Ä—É—Å—Å–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ "11 –Ω–æ—è–±—Ä—è 2025"
        months_ru = {
            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,
            '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
        }

        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: —á–∏—Å–ª–æ + –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—è—Ü–∞ + –≥–æ–¥
        date_match_ru = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', day_date_str.lower())
        if date_match_ru:
            day = int(date_match_ru.group(1))
            month_name = date_match_ru.group(2)
            year = int(date_match_ru.group(3))

            if month_name in months_ru:
                month = months_ru[month_name]
                result = datetime.date(year, month, day)
                logger.debug(f"–†–∞—Å–ø–∞—Ä—Å–µ–Ω–∞ –¥–∞—Ç–∞ –∏–∑ '{day_date_str}': {result}")
                return result

        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY –∏–ª–∏ 'DD –º–µ—Å—è—Ü–∞ YYYY' –≤ —Å—Ç—Ä–æ–∫–µ: '{day_date_str}'")
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞—Ç—ã –∏–∑ '{day_date_str}': {e}")
    return None

async def get_schedule(date_str: str, query_value: str, entity_type: Literal["Group", "Teacher"], use_cache: bool = True) -> Tuple[Optional[List[str]], Optional[str]]:
    if entity_type == API_TYPE_TEACHER:
        url = f"{BASE_URL_SCHEDULE}?teacher={quote(query_value)}&date={date_str}"
        not_found_msg = f"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è '{query_value}' –Ω–∞ {date_str} ü´§"
    elif entity_type == API_TYPE_GROUP:
        url = f"{BASE_URL_SCHEDULE}?date={date_str}&group={query_value}"
        not_found_msg = f"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø—ã '{query_value}' –Ω–∞ {date_str} ü´§"
    else:
        return None, "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞."

    try:
        response = await make_request_with_retry(url, schedule_cache, use_cache=use_cache)
    except Exception as e:
        return None, f"üòî –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.\n\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n‚Ä¢ –°–∞–π—Ç –í–ì–õ–¢–£ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω\n‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º"

    soup = BeautifulSoup(response.text, "lxml")
    # –ò—â–µ–º div —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
    days_html = find_schedule_divs(soup)
    if not days_html:
        return [not_found_msg], None

    # –£–ë–ò–†–ê–ï–ú –°–¢–†–û–ì–£–Æ –§–ò–õ–¨–¢–†–ê–¶–ò–Æ –ü–û –î–ê–¢–ï - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–Ω–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª API
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ª–∏—Å—Ç–∞–Ω–∏—è

    pages: List[str] = []
    for day_div in days_html:
        try:
            date_header = day_div.find("strong")
            day_date_str = date_header.text.strip() if date_header else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –¥–∞—Ç–∞"

            # –£–ë–ò–†–ê–ï–ú –°–¢–†–û–ì–£–Æ –§–ò–õ–¨–¢–†–ê–¶–ò–Æ –ü–û –î–ê–¢–ï - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–Ω–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª API
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ª–∏—Å—Ç–∞–Ω–∏—è
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞, API –≤—Å–µ —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π

            weekday_divs = day_div.find_all("div")
            weekday = weekday_divs[1].text.strip() if len(weekday_divs) > 1 else ""
            pairs_html = day_div.find_all("tr")

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–Ω–∏ –±–µ–∑ –ø–∞—Ä
            if not pairs_html:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã (–Ω–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏)
            has_real_pairs = False
            for pair_tr in pairs_html:
                try:
                    tds = pair_tr.find_all("td")
                    if tds:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —è—á–µ–π–∫–∞—Ö
                        content = "".join([td.text.strip() if td.text else "" for td in tds])
                        if content and content.strip():
                            has_real_pairs = True
                            break
                except Exception:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                    continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–Ω–∏ –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ä
            if not has_real_pairs:
                continue

            day_text = f"<b>{day_date_str} ({weekday})</b>:\n\n"
            last_time_value = ""
            last_counted_time = ""
            pair_counter = 0

            for pair_tr in pairs_html:
                try:
                    tds = pair_tr.find_all("td")
                    if not tds:
                        continue

                    if len(tds) == 1:
                        time = last_time_value or "-"
                        content_td = tds[0]
                        extra_td = None
                    else:
                        time_candidate = tds[0].text.strip() if len(tds) > 0 else ""
                        if time_candidate:
                            last_time_value = time_candidate
                        time = last_time_value or time_candidate or "-"
                        content_td = tds[1] if len(tds) > 1 else tds[0]
                        extra_td = tds[2] if len(tds) > 2 else None

                    if time and time != last_counted_time:
                        pair_counter += 1
                        last_counted_time = time

                    try:
                        details_lines = [line.strip() for line in content_td.text.strip().split("\n") if line.strip()]
                        subject = re.sub(SUBGROUP_PATTERN, r"\1 \2", details_lines[0] if details_lines else "-")
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–µ–¥–º–µ—Ç–∞: {e}")
                        subject = "-"

                    auditorium_link = "-"
                    try:
                        auditorium_a = pair_tr.find("a", href=lambda href: href and "map/rasp?auditory=" in href)
                        if auditorium_a and auditorium_a.has_attr('href'):
                            href = auditorium_a['href']
                            full_href = f"https://vgltu.ru{href}" if not href.startswith('http') else href
                            auditorium_link = f'<a href="{full_href}">{auditorium_a.text.strip()}</a>'
                        elif extra_td is not None and extra_td.text.strip():
                            text = extra_td.text.strip()
                            auditorium_link = f'<a href="https://vgltu.ru/map/rasp?auditory={quote(text)}">{text}</a>'
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏: {e}")

                    try:
                        groups = [p.strip() for p in content_td.decode_contents().split("<br/>") if re.fullmatch(GROUP_NAME_PATTERN, p.strip())]
                        group_names = ", ".join(groups) if groups else "-"
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä—É–ø–ø: {e}")
                        group_names = "-"

                    idx = pair_counter - 1
                    pair_emoji = PAIR_EMOJIS[idx] if 0 <= idx < len(PAIR_EMOJIS) else f" {idx+1}."
                    pair_info = f"{pair_emoji} <b>{time}</b>\n  üìñ {subject}\n  üìç {auditorium_link}\n"

                    if entity_type == API_TYPE_GROUP and len(details_lines) > 1:
                        try:
                            last_line = details_lines[-1]
                            if last_line != subject and not re.fullmatch(GROUP_NAME_PATTERN, last_line):
                                pair_info += f"  üë§ {last_line}\n"
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è: {e}")
                    if group_names != "-":
                        pair_info += f"  üë• {group_names}\n"

                    day_text += pair_info + "\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n"
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä—ã: {e}", exc_info=True)
                    continue

            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Python < 3.9)
            day_text_cleaned = day_text.strip()
            if day_text_cleaned.endswith("‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî"):
                day_text_cleaned = day_text_cleaned[:-len("‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî")].strip()
            pages.append(day_text_cleaned)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–Ω—è: {e}", exc_info=True)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö –¥–Ω–µ–π
            pages.append(f"<b>–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è</b>\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–µ–Ω—å: {str(e)}")

    # –ï—Å–ª–∏ –≤—Å–µ –¥–Ω–∏ –±—ã–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã (–Ω–µ—Ç –¥–Ω–µ–π —Å –ø–∞—Ä–∞–º–∏), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    if not pages:
        return [not_found_msg], None

    return pages, None

def find_schedule_divs(soup: BeautifulSoup) -> List:
    """
    –ù–∞—Ö–æ–¥–∏—Ç div-–±–ª–æ–∫–∏ —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.
    """
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–∏—Å–∫ –ø–æ —Å—Ç–∏–ª—é (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥)
    days_html = soup.find_all("div", style=lambda x: x and "margin-bottom: 25px" in x)
    if days_html:
        return days_html

    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (div > strong —Å –¥–∞—Ç–æ–π)
    # –ò—â–µ–º div, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç strong, —Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Ö–æ–∂ –Ω–∞ –¥–∞—Ç—É
    candidates = []
    for div in soup.find_all("div"):
        strong = div.find("strong", recursive=False)
        if strong:
            text = strong.text.strip()
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã –∏ —Ç–æ—á–∫–∏ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—è—Ü–∞
            if re.search(r'\d{2}\.\d{2}\.\d{4}', text) or \
               re.search(r'\d+\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)', text.lower()):
                candidates.append(div)

    if candidates:
        logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(candidates)} –¥–Ω–µ–π –ø–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (strong tag)")
        return candidates

    return []

async def get_schedule_structured(date_str: str, query_value: str, entity_type: Literal["Group", "Teacher"]) -> Tuple[Optional[Dict], Optional[str]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ (–¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞)
    –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –¥–∞—Ç–∞ –∏–∑ HTML —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–π –¥–∞—Ç–µ,
    —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–∏–≤—è–∑–∫–∏ –ø–∞—Ä –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –¥–Ω—è–º.

    Returns:
        Dict —Å –∫–ª—é—á–∞–º–∏:
        - date: –¥–∞—Ç–∞
        - weekday: –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏
        - pairs: List[Dict] –≥–¥–µ –∫–∞–∂–¥—ã–π Dict —Å–æ–¥–µ—Ä–∂–∏—Ç time, subject, groups, auditorium, teacher
    """
    if entity_type == API_TYPE_TEACHER:
        url = f"{BASE_URL_SCHEDULE}?teacher={quote(query_value)}&date={date_str}"
    elif entity_type == API_TYPE_GROUP:
        url = f"{BASE_URL_SCHEDULE}?date={date_str}&group={query_value}"
    else:
        return None, "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞."

    try:
        response = await make_request_with_retry(url, schedule_cache, use_cache=True)
    except Exception as e:
        return None, "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    soup = BeautifulSoup(response.text, "lxml")

    # –ò—â–µ–º div —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
    days_html = find_schedule_divs(soup)
    if not days_html:
        logger.warning(f"–î–ª—è {date_str} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ div —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º –≤ HTML")
        return None, "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

    # –ü–∞—Ä—Å–∏–º –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –¥–∞—Ç—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    try:
        requested_date = datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
    except Exception:
        return None, f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {date_str}"

    logger.debug(f"üîç –ò—â–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –¥–∞—Ç—ã {date_str} ({requested_date}), –Ω–∞–π–¥–µ–Ω–æ {len(days_html)} –¥–Ω–µ–π –≤ HTML")

    # –ò—â–µ–º –¥–µ–Ω—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–∞—Ç–æ–π (API –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π)
    day_div = None
    day_date_str = None
    weekday = ""

    candidate_by_day = None
    candidate_by_weekday = None

    for div in days_html:
        date_header = div.find("strong")
        if not date_header:
            continue

        html_date_str = date_header.text.strip()
        html_date = parse_date_from_html(html_date_str)

        # –ï—Å–ª–∏ –¥–∞—Ç–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç –¥–µ–Ω—å
        if html_date and html_date == requested_date:
            day_div = div
            day_date_str = html_date_str
            weekday_divs = div.find_all("div")
            weekday = weekday_divs[1].text.strip() if len(weekday_divs) > 1 else ""
            logger.debug(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥–µ–Ω—å —Å —Å–æ–≤–ø–∞–¥–∞—é—â–µ–π –¥–∞—Ç–æ–π: {html_date_str}")
            break
        elif html_date and html_date.day == requested_date.day and html_date.month == requested_date.month and not candidate_by_day:
            candidate_by_day = (div, html_date_str)
        elif html_date and html_date.weekday() == requested_date.weekday() and not candidate_by_weekday:
            candidate_by_weekday = (div, html_date_str)

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–∞—Ç—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –¥–Ω—é –Ω–µ–¥–µ–ª–∏
    if day_div is None:
        if candidate_by_day:
            logger.debug(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–Ω—å —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º —á–∏—Å–ª–æ–º –º–µ—Å—è—Ü–∞: {candidate_by_day[1]}")
            day_div = candidate_by_day[0]
            day_date_str = candidate_by_day[1]
            weekday_divs = day_div.find_all("div")
            weekday = weekday_divs[1].text.strip() if len(weekday_divs) > 1 else ""
        elif candidate_by_weekday:
            logger.debug(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–Ω—å —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º –¥–Ω—ë–º –Ω–µ–¥–µ–ª–∏: {candidate_by_weekday[1]}")
            day_div = candidate_by_weekday[0]
            day_date_str = candidate_by_weekday[1]
            weekday_divs = day_div.find_all("div")
            weekday = weekday_divs[1].text.strip() if len(weekday_divs) > 1 else ""
        elif len(days_html) == 1:
            logger.debug(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–µ–Ω—å –∏–∑ –æ—Ç–≤–µ—Ç–∞: {date_str}")
            day_div = days_html[0]
            date_header = day_div.find("strong")
            day_date_str = date_header.text.strip() if date_header else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –¥–∞—Ç–∞"
            weekday_divs = day_div.find_all("div")
            weekday = weekday_divs[1].text.strip() if len(weekday_divs) > 1 else ""
        else:
            logger.debug(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–µ–Ω—å –¥–ª—è {date_str}. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ.")
            return None, None

    pairs = []
    pairs_html = day_div.find_all("tr")

    # –£–ü–†–û–©–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
    # –ü—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ "–ù–µ—Ç –ø–∞—Ä" –¥–µ–ª–∞–µ–º –ø–æ–∑–∂–µ, –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–∞—Ä
    if not pairs_html:
        logger.info(f"–î–ª—è –¥–∞—Ç—ã {date_str} –Ω–µ—Ç —Å—Ç—Ä–æ–∫ <tr> –≤ —Ç–∞–±–ª–∏—Ü–µ")
        return None, None

    logger.debug(f"–î–ª—è –¥–∞—Ç—ã {date_str} –Ω–∞–π–¥–µ–Ω–æ {len(pairs_html)} —Å—Ç—Ä–æ–∫ <tr> –≤ —Ç–∞–±–ª–∏—Ü–µ")

    # –£–ë–ò–†–ê–ï–ú –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–£–Æ –ü–†–û–í–ï–†–ö–£ - –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
    # –ü—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ "–ù–µ—Ç –ø–∞—Ä" —Å–¥–µ–ª–∞–µ–º –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–∞—Ä

    last_time_value = ""

    for pair_tr in pairs_html:
        tds = pair_tr.find_all("td")
        if not tds:
            continue

        if len(tds) == 1:
            time = last_time_value or "-"
            content_td = tds[0]
            extra_td = None
        else:
            time_candidate = tds[0].text.strip()
            if time_candidate:
                last_time_value = time_candidate
            time = last_time_value or time_candidate or "-"
            content_td = tds[1]
            extra_td = tds[2] if len(tds) > 2 else None

        details_lines = [line.strip() for line in content_td.text.strip().split("\n") if line.strip()]
        subject = re.sub(SUBGROUP_PATTERN, r"\1 \2", details_lines[0] if details_lines else "-")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–∫–∞ "–ù–µ—Ç –ø–∞—Ä"
        row_text_full = "".join([td.text.strip() if td.text else "" for td in tds]).lower().strip()
        if row_text_full in ["–Ω–µ—Ç –ø–∞—Ä", "–Ω–µ—Ç –∑–∞–Ω—è—Ç–∏–π", "–∑–∞–Ω—è—Ç–∏–π –Ω–µ—Ç"]:
            logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É '–ù–µ—Ç –ø–∞—Ä' –¥–ª—è {date_str}")
            continue

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–¥–º–µ—Ç —è–≤–Ω–æ "–ù–µ—Ç –ø–∞—Ä" –∏ –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
        if not time or time == '-':
            if subject and subject.lower().strip() in ["–Ω–µ—Ç –ø–∞—Ä", "–Ω–µ—Ç –∑–∞–Ω—è—Ç–∏–π", "–∑–∞–Ω—è—Ç–∏–π –Ω–µ—Ç"]:
                logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É '–ù–µ—Ç –ø–∞—Ä' –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è {date_str}")
                continue

        auditorium = "-"
        auditorium_a = pair_tr.find("a", href=lambda href: href and "map/rasp?auditory=" in href)
        if auditorium_a:
            auditorium = auditorium_a.text.strip()
        elif extra_td is not None and extra_td.text.strip():
            auditorium = extra_td.text.strip()

        groups = [p.strip() for p in content_td.decode_contents().split("<br/>") if re.fullmatch(GROUP_NAME_PATTERN, p.strip())]

        teacher = ""
        if entity_type == API_TYPE_GROUP and len(details_lines) > 1:
            last_line = details_lines[-1]
            if last_line != subject and not re.fullmatch(GROUP_NAME_PATTERN, last_line):
                teacher = last_line

        pairs.append({
            "time": time,
            "subject": subject,
            "groups": groups,
            "auditorium": auditorium,
            "teacher": teacher,
        })

    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Ç –ø–∞—Ä, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
    if not pairs:
        logger.info(f"–î–ª—è –¥–∞—Ç—ã {date_str} –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Ç –ø–∞—Ä (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(pairs_html)} —Å—Ç—Ä–æ–∫)")
        return None, None

    logger.debug(f"‚úÖ –î–ª—è –¥–∞—Ç—ã {date_str} —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(pairs)} –ø–∞—Ä")

    parsed_day_date = parse_date_from_html(day_date_str) if day_date_str else None
    date_iso = parsed_day_date.isoformat() if parsed_day_date else requested_date.isoformat()

    return {
        "date": day_date_str,
        "date_iso": date_iso,
        "weekday": weekday,
        "pairs": pairs
    }, None

async def search_entities(query: str, entity_type: Literal["Group", "Teacher"]) -> Tuple[Optional[List[str]], Optional[str]]:
    url = f"{BASE_URL_LIST}?type={entity_type}"
    try:
        response = await make_request_with_retry(url, list_cache)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type –∑–∞–≥–æ–ª–æ–≤–∫–∞, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω –Ω–µ json
        content_type = response.headers.get('Content-Type', '') or ''
        content_type_lower = content_type.lower()
        is_json_content_type = 'application/json' in content_type_lower or 'text/json' in content_type_lower

        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å JSON –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞
        try:
            entities = response.json()
        except ValueError as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}, content-type: '{content_type}', response text: {response.text[:200]}"
            )
            return None, "–û—à–∏–±–∫–∞: –°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."

        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π, –Ω–æ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ INFO –±–µ–∑ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
        if not is_json_content_type:
            logger.info(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π Content-Type (–Ω–æ JSON –ø–æ–ª—É—á–µ–Ω): '{content_type}'")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        if not isinstance(entities, list):
            logger.error(f"–û–∂–∏–¥–∞–ª—Å—è list, –ø–æ–ª—É—á–µ–Ω {type(entities)}: {entities}")
            return None, "–û—à–∏–±–∫–∞: –°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã - —Å—Ç—Ä–æ–∫–∏
        if not all(isinstance(item, str) for item in entities):
            logger.error("–ù–µ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ —è–≤–ª—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏")
            return None, "–û—à–∏–±–∫–∞: –°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        filtered = [e for e in entities if query.lower() in e.lower()]
        return filtered if filtered else None, None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}", exc_info=True)
        return None, f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {str(e)}"


